{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Spam/Ham Classifier with PyTorch Neural Networks\n",
    "\n",
    "Welcome! In this tutorial, you'll learn how to build a spam email classifier using deep learning.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Data Loading**: How to read and parse raw email files\n",
    "2. **Text Preprocessing**: Cleaning and preparing text data for machine learning\n",
    "3. **Feature Engineering**: Converting text to numbers (vectorization)\n",
    "4. **Neural Networks**: Building a classifier with PyTorch\n",
    "5. **Training**: How models learn from data\n",
    "6. **Evaluation**: Measuring model performance\n",
    "\n",
    "## Key Concepts Explained Along the Way\n",
    "\n",
    "- What is a neural network?\n",
    "- What are activation functions?\n",
    "- What is overfitting and how to prevent it?\n",
    "- Why dataset size matters for deep learning\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Setup and Imports\n",
    "\n",
    "First, let's import all the libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import email\n",
    "from email import policy\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Loading and Exploring the Data\n",
    "\n",
    "### Understanding the Dataset\n",
    "\n",
    "Our dataset contains raw email files organized into folders:\n",
    "- `email-data-set/ham/hard_ham/` - Legitimate emails (\"ham\")\n",
    "- `email-data-set/spam/spam/` - Spam emails\n",
    "\n",
    "Each file is a complete email with headers (From, To, Subject, etc.) and body content.\n",
    "\n",
    "### Why \"Ham\"?\n",
    "In spam filtering, legitimate emails are called \"ham\" - the opposite of spam. It's a playful term used in the industry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to our data\n",
    "DATA_DIR = Path(\"email-data-set\")\n",
    "HAM_DIR = DATA_DIR / \"ham\" / \"hard_ham\"\n",
    "SPAM_DIR = DATA_DIR / \"spam\" / \"spam\"\n",
    "\n",
    "# Check if directories exist\n",
    "print(f\"Ham directory exists: {HAM_DIR.exists()}\")\n",
    "print(f\"Spam directory exists: {SPAM_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(file_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Parse an email file and extract useful information.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the email file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with subject, from, body, and raw content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try different encodings (emails can be messy!)\n",
    "        for encoding in ['utf-8', 'latin-1', 'ascii', 'cp1252']:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "                    raw_content = f.read()\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        # Parse the email using Python's email library\n",
    "        msg = email.message_from_string(raw_content, policy=policy.default)\n",
    "        \n",
    "        # Extract subject\n",
    "        subject = msg.get('Subject', '')\n",
    "        if subject is None:\n",
    "            subject = ''\n",
    "        \n",
    "        # Extract sender\n",
    "        from_addr = msg.get('From', '')\n",
    "        if from_addr is None:\n",
    "            from_addr = ''\n",
    "        \n",
    "        # Extract body\n",
    "        body = ''\n",
    "        if msg.is_multipart():\n",
    "            # Email has multiple parts (text, HTML, attachments)\n",
    "            for part in msg.walk():\n",
    "                content_type = part.get_content_type()\n",
    "                if content_type == 'text/plain':\n",
    "                    try:\n",
    "                        body = part.get_content()\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "                elif content_type == 'text/html' and not body:\n",
    "                    try:\n",
    "                        body = part.get_content()\n",
    "                    except:\n",
    "                        pass\n",
    "        else:\n",
    "            # Simple email with single body\n",
    "            try:\n",
    "                body = msg.get_content()\n",
    "            except:\n",
    "                body = raw_content\n",
    "        \n",
    "        return {\n",
    "            'subject': str(subject),\n",
    "            'from': str(from_addr),\n",
    "            'body': str(body) if body else '',\n",
    "            'raw': raw_content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # If parsing fails, return raw content\n",
    "        return {\n",
    "            'subject': '',\n",
    "            'from': '',\n",
    "            'body': raw_content if 'raw_content' in dir() else '',\n",
    "            'raw': raw_content if 'raw_content' in dir() else ''\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(directory: Path, label: int) -> list:\n",
    "    \"\"\"\n",
    "    Load all emails from a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory: Path to directory containing email files\n",
    "        label: 0 for ham, 1 for spam\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with email data and labels\n",
    "    \"\"\"\n",
    "    emails = []\n",
    "    \n",
    "    # Get all files in directory (exclude hidden files)\n",
    "    files = [f for f in directory.iterdir() if f.is_file() and not f.name.startswith('.')]\n",
    "    \n",
    "    for file_path in tqdm(files, desc=f\"Loading {'spam' if label else 'ham'}\"):\n",
    "        parsed = parse_email(file_path)\n",
    "        parsed['label'] = label\n",
    "        parsed['filename'] = file_path.name\n",
    "        emails.append(parsed)\n",
    "    \n",
    "    return emails\n",
    "\n",
    "# Load all emails\n",
    "print(\"Loading emails...\")\n",
    "ham_emails = load_emails(HAM_DIR, label=0)  # 0 = ham\n",
    "spam_emails = load_emails(SPAM_DIR, label=1)  # 1 = spam\n",
    "\n",
    "# Combine into a single list\n",
    "all_emails = ham_emails + spam_emails\n",
    "\n",
    "print(f\"\\nLoaded {len(ham_emails)} ham emails\")\n",
    "print(f\"Loaded {len(spam_emails)} spam emails\")\n",
    "print(f\"Total: {len(all_emails)} emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(all_emails)\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts().rename({0: 'Ham', 1: 'Spam'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Dataset\n",
    "\n",
    "Let's see the distribution of our data. Notice we have **class imbalance** - more spam than ham. This is common in real-world datasets and something we'll need to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "labels = ['Ham (Legitimate)', 'Spam']\n",
    "counts = [len(ham_emails), len(spam_emails)]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "axes[0].bar(labels, counts, color=colors)\n",
    "axes[0].set_ylabel('Number of Emails')\n",
    "axes[0].set_title('Email Distribution by Class')\n",
    "for i, v in enumerate(counts):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Class Proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some example emails\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE HAM EMAIL\")\n",
    "print(\"=\" * 60)\n",
    "ham_example = df[df['label'] == 0].iloc[0]\n",
    "print(f\"Subject: {ham_example['subject'][:100]}\")\n",
    "print(f\"From: {ham_example['from'][:50]}\")\n",
    "print(f\"Body (first 500 chars):\\n{ham_example['body'][:500]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE SPAM EMAIL\")\n",
    "print(\"=\" * 60)\n",
    "spam_example = df[df['label'] == 1].iloc[0]\n",
    "print(f\"Subject: {spam_example['subject'][:100]}\")\n",
    "print(f\"From: {spam_example['from'][:50]}\")\n",
    "print(f\"Body (first 500 chars):\\n{spam_example['body'][:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Text Preprocessing\n",
    "\n",
    "### Why Preprocess?\n",
    "\n",
    "Raw text is messy! Emails contain:\n",
    "- HTML tags\n",
    "- Special characters\n",
    "- URLs\n",
    "- Email addresses\n",
    "- Numbers\n",
    "- Mixed capitalization\n",
    "\n",
    "We need to **clean** the text so our model can focus on the meaningful content.\n",
    "\n",
    "### Preprocessing Steps:\n",
    "1. Remove HTML tags\n",
    "2. Convert to lowercase\n",
    "3. Remove URLs and email addresses\n",
    "4. Remove special characters and numbers\n",
    "5. Remove extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and preprocess email text.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw email text\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # 1. Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text(separator=' ')\n",
    "    \n",
    "    # 2. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' url ', text)\n",
    "    \n",
    "    # 4. Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', ' emailaddr ', text)\n",
    "    \n",
    "    # 5. Remove numbers (but keep word tokens like 'free')\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "    \n",
    "    # 6. Remove special characters, keep only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # 7. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our cleaning function\n",
    "sample_text = \"\"\"\n",
    "<html><body>\n",
    "<h1>CONGRATULATIONS! You've WON $1,000,000!!!</h1>\n",
    "<p>Click here: http://spam-link.com to claim your prize!</p>\n",
    "<p>Contact: winner@spam.com for more info.</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"BEFORE cleaning:\")\n",
    "print(sample_text)\n",
    "print(\"\\nAFTER cleaning:\")\n",
    "print(clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all emails\n",
    "# We'll combine subject and body for classification\n",
    "print(\"Cleaning all emails...\")\n",
    "\n",
    "def prepare_email_text(row):\n",
    "    \"\"\"Combine subject and body, then clean.\"\"\"\n",
    "    combined = f\"{row['subject']} {row['body']}\"\n",
    "    return clean_text(combined)\n",
    "\n",
    "df['clean_text'] = df.apply(prepare_email_text, axis=1)\n",
    "\n",
    "# Check for empty texts\n",
    "empty_count = (df['clean_text'].str.len() == 0).sum()\n",
    "print(f\"Emails with empty cleaned text: {empty_count}\")\n",
    "\n",
    "# Remove emails with empty text\n",
    "df = df[df['clean_text'].str.len() > 0].reset_index(drop=True)\n",
    "print(f\"Remaining emails after removing empty: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "df['text_length'] = df['clean_text'].str.len()\n",
    "df['word_count'] = df['clean_text'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Text length distribution by class\n",
    "for label, color, name in [(0, '#2ecc71', 'Ham'), (1, '#e74c3c', 'Spam')]:\n",
    "    subset = df[df['label'] == label]['text_length']\n",
    "    axes[0].hist(subset, bins=50, alpha=0.6, color=color, label=name)\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 5000)  # Limit x-axis for visibility\n",
    "\n",
    "# Word count distribution\n",
    "for label, color, name in [(0, '#2ecc71', 'Ham'), (1, '#e74c3c', 'Spam')]:\n",
    "    subset = df[df['label'] == label]['word_count']\n",
    "    axes[1].hist(subset, bins=50, alpha=0.6, color=color, label=name)\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Word Count Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 1000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nText statistics by class:\")\n",
    "print(df.groupby('label')[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Feature Engineering - Converting Text to Numbers\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Neural networks work with **numbers**, not text. We need to convert our cleaned text into numerical vectors.\n",
    "\n",
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is a technique that converts text into numbers based on:\n",
    "\n",
    "1. **Term Frequency (TF)**: How often a word appears in a document\n",
    "2. **Inverse Document Frequency (IDF)**: How rare a word is across all documents\n",
    "\n",
    "**Key Insight**: Words that appear frequently in one email but rarely in others are more important for classification.\n",
    "\n",
    "For example:\n",
    "- \"the\" appears everywhere → low importance\n",
    "- \"viagra\" appears only in spam → high importance for spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets FIRST\n",
    "# This is crucial to prevent data leakage!\n",
    "\n",
    "X = df['clean_text'].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split: 80% training, 20% testing\n",
    "# stratify=y ensures both sets have similar class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} emails\")\n",
    "print(f\"Test set: {len(X_test)} emails\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(f\"  Ham: {sum(y_train == 0)} ({100*sum(y_train == 0)/len(y_train):.1f}%)\")\n",
    "print(f\"  Spam: {sum(y_train == 1)} ({100*sum(y_train == 1)/len(y_train):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "# max_features limits vocabulary size (helps prevent overfitting)\n",
    "# ngram_range=(1,2) includes single words and pairs of words\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,      # Use top 5000 most important words\n",
    "    ngram_range=(1, 2),     # Include single words and word pairs\n",
    "    min_df=2,               # Word must appear in at least 2 documents\n",
    "    max_df=0.95,            # Ignore words in more than 95% of documents\n",
    "    stop_words='english'    # Remove common English words (the, is, at, etc.)\n",
    ")\n",
    "\n",
    "# Fit on training data ONLY, then transform both\n",
    "# This prevents \"data leakage\" from test set\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"Training matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Test matrix shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the most important words (features)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Calculate average TF-IDF score for each class\n",
    "ham_mask = y_train == 0\n",
    "spam_mask = y_train == 1\n",
    "\n",
    "ham_mean = np.array(X_train_tfidf[ham_mask].mean(axis=0)).flatten()\n",
    "spam_mean = np.array(X_train_tfidf[spam_mask].mean(axis=0)).flatten()\n",
    "\n",
    "# Find words most associated with each class\n",
    "diff = spam_mean - ham_mean\n",
    "\n",
    "# Top spam words (highest positive difference)\n",
    "spam_indices = diff.argsort()[-15:][::-1]\n",
    "print(\"Top words associated with SPAM:\")\n",
    "for idx in spam_indices:\n",
    "    print(f\"  {feature_names[idx]}: {diff[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop words associated with HAM:\")\n",
    "ham_indices = diff.argsort()[:15]\n",
    "for idx in ham_indices:\n",
    "    print(f\"  {feature_names[idx]}: {diff[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Building the Neural Network\n",
    "\n",
    "### What is a Neural Network?\n",
    "\n",
    "A neural network is a series of mathematical operations that learn patterns from data. Think of it like this:\n",
    "\n",
    "```\n",
    "Input (TF-IDF features) → Hidden Layers → Output (spam/ham probability)\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Layers**: Groups of \"neurons\" that process information\n",
    "2. **Weights**: Numbers that the network learns to adjust\n",
    "3. **Activation Functions**: Add non-linearity (allow learning complex patterns)\n",
    "4. **Dropout**: Randomly disable neurons during training (prevents overfitting)\n",
    "\n",
    "### Our Architecture:\n",
    "\n",
    "```\n",
    "Input (5000 features)\n",
    "    ↓\n",
    "Dense Layer (256 neurons) + ReLU + Dropout\n",
    "    ↓\n",
    "Dense Layer (128 neurons) + ReLU + Dropout\n",
    "    ↓\n",
    "Dense Layer (64 neurons) + ReLU + Dropout\n",
    "    ↓\n",
    "Output Layer (1 neuron) + Sigmoid → Probability of Spam\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A feedforward neural network for spam classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: accepts TF-IDF features\n",
    "    - 3 hidden layers with ReLU activation and dropout\n",
    "    - Output layer with sigmoid activation for binary classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, dropout_rate: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize the network.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Number of input features (vocabulary size)\n",
    "            dropout_rate: Probability of dropping neurons (0.3 = 30%)\n",
    "        \"\"\"\n",
    "        super(SpamClassifier, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.network = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(input_dim, 256),  # input_dim -> 256 neurons\n",
    "            nn.ReLU(),                   # Activation function\n",
    "            nn.Dropout(dropout_rate),    # Regularization\n",
    "            \n",
    "            # Second hidden layer\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Third hidden layer\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Output probability between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 1) with spam probabilities\n",
    "        \"\"\"\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "input_dim = X_train_tfidf.shape[1]  # Number of TF-IDF features\n",
    "model = SpamClassifier(input_dim=input_dim, dropout_rate=0.3)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Key Concepts\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**:\n",
    "- Formula: `f(x) = max(0, x)`\n",
    "- Simply: if input is negative, output is 0; otherwise, output equals input\n",
    "- Why use it? Adds non-linearity so the network can learn complex patterns\n",
    "\n",
    "**Dropout**:\n",
    "- Randomly sets some neuron outputs to 0 during training\n",
    "- Prevents the network from relying too much on specific neurons\n",
    "- Acts as regularization to prevent overfitting\n",
    "\n",
    "**Sigmoid**:\n",
    "- Formula: `f(x) = 1 / (1 + e^(-x))`\n",
    "- Squashes output to range [0, 1]\n",
    "- Perfect for binary classification: output is probability of spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Training the Model\n",
    "\n",
    "### The Training Process\n",
    "\n",
    "1. **Forward Pass**: Feed data through the network, get predictions\n",
    "2. **Calculate Loss**: Measure how wrong the predictions are\n",
    "3. **Backward Pass**: Calculate gradients (how to adjust weights)\n",
    "4. **Update Weights**: Adjust weights to reduce loss\n",
    "5. **Repeat** for many iterations (epochs)\n",
    "\n",
    "### Key Training Components:\n",
    "\n",
    "- **Loss Function**: Binary Cross-Entropy (measures prediction error)\n",
    "- **Optimizer**: Adam (efficiently adjusts weights)\n",
    "- **Batch Size**: Process 32 emails at a time\n",
    "- **Epochs**: Complete passes through the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_tfidf.toarray()).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_tfidf.toarray()).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "\n",
    "# Binary Cross-Entropy Loss\n",
    "# - Measures difference between predicted probability and true label\n",
    "# - Lower is better\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Adam optimizer\n",
    "# - lr (learning rate): how big steps to take when adjusting weights\n",
    "# - weight_decay: L2 regularization to prevent overfitting\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode (enables dropout)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Zero gradients from previous batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass (calculate gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset.\n",
    "    \n",
    "    Returns:\n",
    "        loss: Average loss\n",
    "        accuracy: Classification accuracy\n",
    "        predictions: Model predictions\n",
    "        labels: True labels\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout)\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Convert probabilities to predictions (threshold = 0.5)\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "# Early stopping: stop if test loss doesn't improve\n",
    "best_test_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Evaluate on both sets\n",
    "    _, train_acc, _, _ = evaluate(model, train_loader, criterion)\n",
    "    test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "# Restore best model\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nRestored best model (test loss: {best_test_loss:.4f})\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Training Loss', color='blue')\n",
    "axes[0].plot(history['test_loss'], label='Test Loss', color='red')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Test Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Training Accuracy', color='blue')\n",
    "axes[1].plot(history['test_acc'], label='Test Accuracy', color='red')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Training Curves\n",
    "\n",
    "**What to look for:**\n",
    "\n",
    "1. **Both losses decreasing**: Model is learning!\n",
    "2. **Train loss << Test loss (big gap)**: Overfitting - model memorizes training data\n",
    "3. **Both losses similar**: Good generalization\n",
    "4. **Losses plateau**: Model has learned what it can\n",
    "\n",
    "**Overfitting** happens when:\n",
    "- Training accuracy is high but test accuracy is lower\n",
    "- The model memorizes training data instead of learning general patterns\n",
    "- Common with small datasets (like ours!)\n",
    "\n",
    "**Solutions we used:**\n",
    "- Dropout layers\n",
    "- Weight decay (L2 regularization)\n",
    "- Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Evaluating the Model\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "- **Accuracy**: % of correct predictions (can be misleading with imbalanced data)\n",
    "- **Precision**: Of emails predicted as spam, how many were actually spam?\n",
    "- **Recall**: Of actual spam emails, how many did we catch?\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### For Spam Detection:\n",
    "- **High Precision** = Few false positives (legitimate emails marked as spam)\n",
    "- **High Recall** = Catch most spam (few spam emails get through)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "test_loss, test_acc, y_pred, y_true = evaluate(model, test_loader, criterion)\n",
    "\n",
    "# Flatten predictions\n",
    "y_pred = y_pred.flatten()\n",
    "y_true = y_true.flatten()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Ham', 'Spam'],\n",
    "            yticklabels=['Ham', 'Spam'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Interpret the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Interpretation:\")\n",
    "print(f\"  True Negatives (Ham correctly identified): {tn}\")\n",
    "print(f\"  False Positives (Ham incorrectly marked as Spam): {fp}\")\n",
    "print(f\"  False Negatives (Spam that got through): {fn}\")\n",
    "print(f\"  True Positives (Spam correctly caught): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Making Predictions on New Emails\n",
    "\n",
    "Let's see how to use our trained model to classify new emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_email(text: str, model, tfidf_vectorizer, device, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Predict if an email is spam or ham.\n",
    "    \n",
    "    Args:\n",
    "        text: Email text (subject + body)\n",
    "        model: Trained PyTorch model\n",
    "        tfidf_vectorizer: Fitted TF-IDF vectorizer\n",
    "        device: torch device (cpu/cuda)\n",
    "        threshold: Classification threshold (default 0.5)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction and probability\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    features = tfidf_vectorizer.transform([cleaned]).toarray()\n",
    "    features_tensor = torch.FloatTensor(features).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probability = model(features_tensor).item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'SPAM' if probability >= threshold else 'HAM',\n",
    "        'spam_probability': probability,\n",
    "        'confidence': max(probability, 1 - probability)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some example emails\n",
    "\n",
    "test_emails = [\n",
    "    {\n",
    "        'name': 'Obvious Spam',\n",
    "        'text': '''\n",
    "            CONGRATULATIONS! You have been selected to receive $1,000,000!\n",
    "            Click here NOW to claim your prize! Limited time offer!\n",
    "            ACT NOW! FREE MONEY! You're a winner!\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        'name': 'Legitimate Work Email',\n",
    "        'text': '''\n",
    "            Hi team,\n",
    "            \n",
    "            Just a reminder that our meeting has been moved to 3pm tomorrow.\n",
    "            Please review the attached agenda before the meeting.\n",
    "            \n",
    "            Thanks,\n",
    "            John\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        'name': 'Newsletter',\n",
    "        'text': '''\n",
    "            Your Weekly Tech Newsletter\n",
    "            \n",
    "            This week in tech: Apple announces new products, \n",
    "            Google updates search algorithm, and more.\n",
    "            \n",
    "            Click to read more articles on our website.\n",
    "            Unsubscribe from this newsletter.\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phishing Attempt',\n",
    "        'text': '''\n",
    "            URGENT: Your account has been compromised!\n",
    "            \n",
    "            We detected suspicious activity on your account.\n",
    "            Click here immediately to verify your identity and\n",
    "            prevent your account from being suspended.\n",
    "            \n",
    "            Enter your password and credit card to confirm.\n",
    "        '''\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING MODEL ON NEW EMAILS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for email_test in test_emails:\n",
    "    result = predict_email(email_test['text'], model, tfidf, device)\n",
    "    print(f\"\\n{email_test['name']}:\")\n",
    "    print(f\"  Prediction: {result['prediction']}\")\n",
    "    print(f\"  Spam Probability: {result['spam_probability']:.2%}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Saving the Model\n",
    "\n",
    "Let's save our trained model and vectorizer so we can use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save PyTorch model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_dim': input_dim,\n",
    "    'dropout_rate': 0.3\n",
    "}, models_dir / 'spam_classifier.pth')\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "with open(models_dir / 'tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "print(\"Model saved to 'models/' directory!\")\n",
    "print(f\"  - spam_classifier.pth ({(models_dir / 'spam_classifier.pth').stat().st_size / 1024:.1f} KB)\")\n",
    "print(f\"  - tfidf_vectorizer.pkl ({(models_dir / 'tfidf_vectorizer.pkl').stat().st_size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "A neural network spam classifier using PyTorch that:\n",
    "1. Loads and parses raw email files\n",
    "2. Cleans and preprocesses text\n",
    "3. Converts text to TF-IDF features\n",
    "4. Classifies emails as spam or ham\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Text Preprocessing**: Essential for NLP - clean, normalize, and tokenize text\n",
    "\n",
    "2. **TF-IDF Vectorization**: Convert text to numbers while preserving importance\n",
    "\n",
    "3. **Neural Network Architecture**:\n",
    "   - Input layer matches feature dimensions\n",
    "   - Hidden layers learn patterns\n",
    "   - Activation functions add non-linearity\n",
    "   - Output layer produces predictions\n",
    "\n",
    "4. **Training Process**:\n",
    "   - Forward pass → Loss calculation → Backward pass → Weight update\n",
    "   - Monitor both training and validation metrics\n",
    "\n",
    "5. **Regularization**:\n",
    "   - Dropout prevents overfitting\n",
    "   - Early stopping prevents overtraining\n",
    "\n",
    "6. **Evaluation Metrics**:\n",
    "   - Accuracy alone isn't enough for imbalanced datasets\n",
    "   - Precision, recall, and F1 give fuller picture\n",
    "\n",
    "### Limitations and Improvements\n",
    "\n",
    "Our model has some limitations:\n",
    "- **Small dataset** (752 emails): Deep learning typically needs more data\n",
    "- **Simple architecture**: Could try LSTM or Transformers for better results\n",
    "- **TF-IDF features**: Word embeddings (Word2Vec, BERT) could capture more meaning\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To improve, you could:\n",
    "1. Collect more training data\n",
    "2. Try pre-trained embeddings (GloVe, BERT)\n",
    "3. Experiment with recurrent networks (LSTM, GRU)\n",
    "4. Implement cross-validation for more robust evaluation\n",
    "5. Add data augmentation techniques\n",
    "\n",
    "Congratulations on building your first spam classifier!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
